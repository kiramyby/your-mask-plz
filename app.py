import os
import tempfile

import cv2
import streamlit as st
from PIL import Image
from ultralytics.models import YOLO

# Page config
st.set_page_config(
    page_title="Your Mask Plz - Detection App",
    page_icon="üò∑",
    layout="wide"
)

lang = st.sidebar.radio("Language / ËØ≠Ë®Ä", ("English", "‰∏≠Êñá"), horizontal=True)

TEXT = {
    "title": {"English": "üò∑ Your Mask Plz: Face Mask Detection", "‰∏≠Êñá": "üò∑ Your Mask Plz Âè£ÁΩ©Ê£ÄÊµã"},
    "intro": {
        "English": "Upload an image or video to detect mask wearing (YOLOv11n ONNX).",
        "‰∏≠Êñá": "‰∏ä‰º†ÂõæÁâáÊàñËßÜÈ¢ëÔºåËøõË°åÂè£ÁΩ©‰Ω©Êà¥Ê£ÄÊµãÔºàÂü∫‰∫é YOLOv11n ONNXÔºâ„ÄÇ",
    },
    "settings": {"English": "Settings", "‰∏≠Êñá": "ËÆæÁΩÆ"},
    "conf": {"English": "Confidence Threshold", "‰∏≠Êñá": "ÁΩÆ‰ø°Â∫¶ÈòàÂÄº"},
    "model": {"English": "Model Path", "‰∏≠Êñá": "Ê®°ÂûãË∑ØÂæÑ"},
    "frame_step": {"English": "Video frame step (skip N-1 frames)", "‰∏≠Êñá": "ËßÜÈ¢ëÊäΩÂ∏ßÊ≠•Èïø (Ë∑≥Ëøá N-1 Â∏ß)"},
    "loaded": {"English": "Model loaded", "‰∏≠Êñá": "Ê®°ÂûãÂä†ËΩΩÊàêÂäü"},
    "upload": {"English": "Choose image or video...", "‰∏≠Êñá": "ÈÄâÊã©ÂõæÁâáÊàñËßÜÈ¢ë..."},
    "orig_img": {"English": "Original", "‰∏≠Êñá": "ÂéüÂßãÂõæÁâá"},
    "detect_btn": {"English": "Detect", "‰∏≠Êñá": "ÂºÄÂßãÊ£ÄÊµã"},
    "detecting": {"English": "Detecting...", "‰∏≠Êñá": "Ê£ÄÊµã‰∏≠..."},
    "result_img": {"English": "Result", "‰∏≠Êñá": "Ê£ÄÊµãÁªìÊûú"},
    "no_detect": {"English": "No mask/face detected.", "‰∏≠Êñá": "Êú™Ê£ÄÊµãÂà∞Âè£ÁΩ©/Êú™‰Ω©Êà¥Âè£ÁΩ©ÁõÆÊ†á„ÄÇ"},
    "count": {"English": "Detected {n} objects, counts: {c}", "‰∏≠Êñá": "Ê£ÄÊµãÂà∞ {n} ‰∏™ÁõÆÊ†áÔºåÁ±ªÂà´ËÆ°Êï∞Ôºö{c}"},
    "orig_vid": {"English": "Original Video", "‰∏≠Êñá": "ÂéüÂßãËßÜÈ¢ë"},
    "result_vid": {"English": "Detection", "‰∏≠Êñá": "Ê£ÄÊµãÁªìÊûú"},
    "detect_vid_btn": {"English": "Detect Video", "‰∏≠Êñá": "ÂºÄÂßãÊ£ÄÊµãËßÜÈ¢ë"},
    "video_done": {"English": "Video processed!", "‰∏≠Êñá": "ËßÜÈ¢ëÂ§ÑÁêÜÂÆåÊàêÔºÅ"},
    "video_summary": {
        "English": "Processed frames: {p}, shown: {s}, class counts: {c}",
        "‰∏≠Êñá": "Â∑≤Â§ÑÁêÜÂ∏ßÊï∞Ôºö{p}ÔºåÂ±ïÁ§∫Â∏ßÊï∞Ôºö{s}ÔºåÁ±ªÂà´Á¥ØËÆ°Ôºö{c}",
    },
    "no_frame": {"English": "Cannot read video frame", "‰∏≠Êñá": "Êó†Ê≥ïËØªÂèñËßÜÈ¢ëÂ∏ß"},
    "model_missing": {"English": "Model not found: {p}", "‰∏≠Êñá": "Êú™ÊâæÂà∞Ê®°ÂûãÊñá‰ª∂Ôºö{p}"},
    "model_err": {"English": "Load model error: {e}", "‰∏≠Êñá": "Âä†ËΩΩÊ®°ÂûãÂá∫ÈîôÔºö{e}"},
    "model_warn": {"English": "Model missing. Check path.", "‰∏≠Êñá": "Êú™ËÉΩÂä†ËΩΩÊ®°ÂûãÔºåËØ∑Ê£ÄÊü•Ê®°ÂûãË∑ØÂæÑÊòØÂê¶Ê≠£Á°Æ„ÄÇ"},
}

st.title(TEXT["title"][lang])
st.markdown(TEXT["intro"][lang])

# Sidebar settings
st.sidebar.header(TEXT["settings"][lang])
conf_threshold = st.sidebar.slider(TEXT["conf"][lang], 0.0, 1.0, 0.25, 0.05)
model_path = st.sidebar.text_input(TEXT["model"][lang], "runs/detect/train/weights/best.onnx")
video_frame_step = st.sidebar.slider(TEXT["frame_step"][lang], 1, 5, 1, 1)

# Ê†πÊçÆËæìÂÖ•Â∞∫ÂØ∏Ëá™ÈÄÇÂ∫îÊé®ÁêÜ‰∏éÂ±ïÁ§∫ÂèÇÊï∞
def choose_imgsz(width: int, height: int) -> int:
    """‰æùÊçÆÊúÄÁü≠ËæπÈÄâÊã©Êé®ÁêÜÂ∞∫ÂØ∏ÔºåÈÅøÂÖçËøáÂ§ßÂàÜËæ®ÁéáÊãñÊÖ¢Êé®ÁêÜ„ÄÇ"""
    short = min(width, height)
    if short >= 1600:
        return 960
    if short >= 960:
        return 768
    return 640

# ËΩΩÂÖ•Ê®°ÂûãÔºàÁºìÂ≠ò‰ª•ÈÅøÂÖçÈáçÂ§çÂä†ËΩΩÔºâ
@st.cache_resource
def load_model(path: str):
    if not os.path.exists(path):
        st.error(TEXT["model_missing"][lang].format(p=path))
        return None
    try:
        return YOLO(path, task="detect")
    except Exception as e:  # noqa: BLE001
        st.error(TEXT["model_err"][lang].format(e=e))
        return None

model = load_model(model_path)

# Main content
if model:
    st.sidebar.success(TEXT["loaded"][lang])

    # Êñá‰ª∂‰∏ä‰º†
    uploaded_file = st.file_uploader(
        TEXT["upload"][lang], type=["jpg", "jpeg", "png", "mp4", "mov", "avi"]
    )

    if uploaded_file is not None:
        file_type = uploaded_file.type.split("/")[0]

        if file_type == "image":
            # ËØªÂèñÂõæÁâáÂπ∂ËΩ¨Êç¢‰∏∫ RGB
            image = Image.open(uploaded_file).convert("RGB")
            img_w, img_h = image.size
            imgsz = choose_imgsz(img_w, img_h)
            # ‰æùÊçÆÂéüÂõæÂÆΩÂ∫¶ËÆæÂÆöÂ±ïÁ§∫ÂÆΩÂ∫¶Ôºà‰∏çËÆ©ËøáÂ§ß/ËøáÂ∞èÔºâ
            display_w = min(max(img_w, 480), 1200)

            st.subheader("ÂõæÁâáËæìÂÖ• / ËæìÂá∫" if lang == "‰∏≠Êñá" else "Image Input / Output")
            col_in, col_out = st.columns(2)
            with col_in:
                st.caption(TEXT["orig_img"][lang])
                st.image(image, width=display_w)

            if st.button(TEXT["detect_btn"][lang], type="primary"):
                with st.spinner(TEXT["detecting"][lang]):
                    # Êé®ÁêÜ
                    results = model.predict(image, conf=conf_threshold, imgsz=imgsz)
                    # Ë∞ÉÊï¥Ê°ÜÁ∫ø‰∏éÊñáÂ≠óÂ§ßÂ∞èÔºåÈÅøÂÖçÈÅÆÊå°
                    res_plotted = results[0].plot(line_width=1, font_size=10)
                    with col_out:
                        st.caption(f"{TEXT['result_img'][lang]} (imgsz={imgsz})")
                        st.image(res_plotted, width=display_w)

                    # ÁªüËÆ°Ê£ÄÊµãÊï∞Èáè
                    boxes = results[0].boxes
                    if boxes:
                        names = model.names
                        cls_counts = {}
                        for c in boxes.cls:
                            c_name = names[int(c)]
                            cls_counts[c_name] = cls_counts.get(c_name, 0) + 1
                        st.info(TEXT["count"][lang].format(n=len(boxes), c=cls_counts))
                    else:
                        st.warning(TEXT["no_detect"][lang])

        elif file_type == "video":
            # Â∞Ü‰∏ä‰º†ÁöÑËßÜÈ¢ë‰øùÂ≠òÂà∞‰∏¥Êó∂Êñá‰ª∂
            tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
            tfile.write(uploaded_file.read())
            video_path = tfile.name

            st.video(video_path)

            if st.button(TEXT["detect_vid_btn"][lang], type="primary"):
                col_vid_in, col_vid_out = st.columns(2)
                with col_vid_in:
                    st.caption(TEXT["orig_vid"][lang])
                    st.video(video_path)
                with col_vid_out:
                    st.caption(TEXT["result_vid"][lang])
                    st_frame = st.empty()
                cap = cv2.VideoCapture(video_path)

                progress_bar = st.progress(0)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                frame_count = 0
                shown_count = 0
                # Á±ªÂà´ËÆ°Êï∞Á¥ØÁßØ
                names = model.names
                cls_counts = {names[i]: 0 for i in names}
                processed_frames = 0

                # ËØªÂèñÈ¶ñÂ∏ßËé∑ÂèñÂ∞∫ÂØ∏ÔºåËá™Âä®ÂÜ≥ÂÆöÊé®ÁêÜÂ∞∫ÂØ∏‰∏éÂ±ïÁ§∫ÂÆΩÂ∫¶
                first_ret, first_frame = cap.read()
                if first_ret:
                    h0, w0, _ = first_frame.shape
                    video_imgsz = choose_imgsz(w0, h0)
                    display_w = min(max(w0, 480), 1200)
                    # È¶ñÂ∏ßÊé®ÂõûÈòüÂàóÂºÄÂ§¥ÔºåÊñπ‰æø‰∏ªÂæ™ÁéØÁªü‰∏ÄÂ§ÑÁêÜ
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                else:
                    st.error(TEXT["no_frame"][lang])
                    cap.release()
                    # Êó†Ê≥ïËØªÂèñÈ¶ñÂ∏ßÔºåÁõ¥Êé•ÈÄÄÂá∫ËßÜÈ¢ëÂ§ÑÁêÜÂàÜÊîØ
                    st.stop()

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    frame_count += 1
                    if total_frames > 0:
                        progress_bar.progress(min(frame_count / total_frames, 1.0))

                    # ÊåâÊ≠•ÈïøÊäΩÂ∏ßÔºåÂáèÂ∞ëË¥üËΩΩÔºàÁî®Êà∑ÂèØË∞ÉÔºâ
                    if frame_count % video_frame_step != 0:
                        continue

                    # Á≠âÊØîÁº©ÊîæÂà∞ËÆæÂÆöÁöÑÊúÄÁü≠ËæπÔºåÈôç‰ΩéËÆ°ÁÆóÈáè
                    h, w, _ = frame.shape
                    scale = video_imgsz / min(h, w)
                    if scale != 1:
                        frame = cv2.resize(frame, (int(w * scale), int(h * scale)))

                    # BGR -> RGBÔºåÂÜçÊé®ÁêÜ
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    results = model.predict(
                        frame_rgb,
                        conf=conf_threshold,
                        imgsz=video_imgsz,
                        verbose=False,
                    )
                    res_plotted = results[0].plot(line_width=1, font_size=10)

                    # Á¥ØÁßØÁ±ªÂà´ËÆ°Êï∞
                    boxes = results[0].boxes
                    if boxes:
                        for c in boxes.cls:
                            cls_name = names[int(c)]
                            cls_counts[cls_name] = cls_counts.get(cls_name, 0) + 1

                    processed_frames += 1

                    shown_count += 1
                    st_frame.image(
                        res_plotted,
                        caption=f"Frame {frame_count} (shown {shown_count})",
                        width=display_w,
                    )

                cap.release()
                st.success(TEXT["video_done"][lang])
                st.info(
                    TEXT["video_summary"][lang].format(
                        p=processed_frames, s=shown_count, c=cls_counts
                    )
                )
else:
    st.warning(TEXT["model_warn"][lang])
